# training-free-methods
This is a repository to collect recent training-free algorithms that can run on a single GPU with no more than 24GB of memory.

## Model Acceleration

**PFDiff: Training-free Acceleration of Diffusion Models through the Gradient Guidance of Past and Future.**<br>
*Guangyi Wang, Yuren Cai, Lijiang Li, Wei Peng, Songzhi Su.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2408.08822)]<br>

## Multimodal Large Language Models

**ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models.**<br>
*Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Xiaoshuai Sun, Rongrong Ji.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2407.21534)] [[Code]](https://github.com/mrwu-mac/ControlMLLM)<br>

**Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs.**<br>
*Shi Liu, Kecheng Zheng, Wei Chen.*<br>
ECCV 2024. [[PDF](https://arxiv.org/abs/2407.21771)] [[Project](https://lalbj.github.io/projects/PAI/)] [[Code]](https://github.com/LALBJ/PAI)<br>

## Material Transfer

**ZeST: Zero-Shot Material Transfer from a Single Image.**<br>
*Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, Varun Jampani.*<br>
ECCV 2024. [[PDF](https://arxiv.org/pdf/2404.06425)] [[Project](https://ttchengab.github.io/zest/)] [[Code]](https://github.com/ttchengab/zest_code)<br>

## Style Transfer

**Artist: Aesthetically Controllable Text-Driven Stylization without Training.**<br>
*Ruixiang Jiang, Changwen Chen.*<br>
arXiv 2024. [[PDF](https://arxiv.org/abs/2407.15842)] [[Project](https://diffusionartist.github.io/)] [[Code]](https://github.com/songrise/artist)<br>

**Visual Style Prompting with Swapping Self-Attention.**<br>
*Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2402.12974)] [[Project](https://curryjung.github.io/VisualStylePrompt/)] [[Code]](https://github.com/naver-ai/Visual-Style-Prompting)<br>

**FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion Models.**<br>
*Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2401.15636)] [[Project](https://freestylefreelunch.github.io/)] [[Code]](https://github.com/FreeStyleFreeLunch/FreeStyle)<br>

**Eye-for-an-eye: Appearance Transfer with Semantic Correspondence in Diffusion Models.**<br>
*Sooyeon Go, Kyungmook Choi, Minjung Shin, Youngjung Uh.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.07008)] [[Project](https://sooyeon-go.github.io/eye_for_an_eye/)] [[Code]](https://github.com/sooyeon-go/eye_for_an_eye)<br>

**Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance.**<br>
*Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou.*<br>
NeurIPS 2024. [[PDF](https://arxiv.org/pdf/2406.07540)] [[Project](https://genforce.github.io/ctrl-x/)] [[Code]](https://github.com/genforce/ctrl-x)<br>

**RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control.**<br>
*Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2405.17401)] [[Project](https://rb-modulation.github.io/)] [[Code]](https://github.com/LituRout/RB-Modulation)<br>

**Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer.**<br>
*Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong Gu, Wen Li, Lixin Duan.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.06835.pdf)]<br>

**Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer.**<br>
*Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2312.09008.pdf)] [[Project](https://jiwoogit.github.io/StyleID_site/)] [[Code]](https://github.com/jiwoogit/StyleID)<br>

## Image Generation

**Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis.**<br>
*Aishwarya Agarwal, Srikrishna Karanam, Balaji Vasan Srinivasan.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2409.02429)]<br>

**Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention.**<br>
*Susung Hong.*<br>
NeurIPS 2024. [[PDF](https://arxiv.org/abs/2408.00760)] [[Code]](https://github.com/SusungHong/SEG-SDXL)<br>

**MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning.**<br>
*Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2408.11001)] [[Project](https://haoningwu3639.github.io/MegaFusion/)] [[Code]](https://github.com/ShaochengShen/MegaFusion/)<br>

**DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis through Structure Guidance.**<br>
*Younghyun Kim, Geunmin Hwang, Junyu Zhang, Eunbyung Park.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2406.18459)] [[Project](https://yhyun225.github.io/DiffuseHigh/)] [[Code]](https://github.com/yhyun225/DiffuseHigh)<br>

**TraDiffusion: Trajectory-Based Training-Free Image Generation.**<br>
*Mingrui Wu, Oucheng Huang, Jiayi Ji, Jiale Li, Xinyue Cai, Huafeng Kuang, Jianzhuang Liu, Xiaoshuai Sun, Rongrong Ji.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2408.09739)] [[Code]](https://github.com/och-mac/TraDiffusion)<br>

**MagicFace: Training-free Universal-Style Human Image Customized Synthesis.**<br>
*Yibin Wang, Weizhong Zhang, Cheng Jin.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2408.07433)] [[Project](https://codegoat24.github.io/MagicFace/)] [[Code]](https://github.com/CodeGoat24/MagicFace)<br>

**AccDiffusion: An Accurate Method for Higher-Resolution Image Generation.**<br>
*Zhihang Lin, Mingbao Lin, Meng Zhao, Rongrong Ji.*<br>
ECCV 2024. [[PDF](https://arxiv.org/abs/2407.10738v2)] [[Project](https://lzhxmu.github.io/accdiffusion/accdiffusion.html)] [[Code]](https://github.com/lzhxmu/AccDiffusion)<br>

**ResMaster: Mastering High-Resolution Image Generation via Structural and Fine-Grained Guidance.**<br>
*Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, Yinqiang Zheng.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.16476)] [[Project](https://shuweis.github.io/ResMaster/)] [[Code]](https://github.com/Shuweis/ResMaster)<br>

**Coherent Zero-Shot Visual Instruction Generation.**<br>
*Quynh Phung, Songwei Ge, Jia-Bin Huang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.04337)] [[Project](https://instruct-vis-zero.github.io/)]<br>

**FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition.**<br>
*Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, Chunhua Shen.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2405.13870)] [[Code]](https://github.com/aim-uofa/FreeCustom)<br>

**Training-free Subject-Enhanced Attention Guidance for Compositional Text-to-image Generation.**<br>
*Shengyuan Liu, Bo Wang, Ye Ma, Te Yang, Xipeng Cao, Quan Chen, Han Li, Di Dong, Peng Jiang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2405.06948)]<br>

**DemoFusion: Democratising High-Resolution Image Generation With No $$$.**<br>
*Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2311.16973.pdf)] [[Project]](https://ruoyidu.github.io/demofusion/demofusion.html) [[Code]](https://github.com/PRIS-CV/DemoFusion)<br>

**HiDiffusion: Unlocking High-Resolution Creativity and Efficiency in Low-Resolution Trained Diffusion Models.**<br>
*Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, Wengang Cao, Jiajun Liang.*<br>
ECCV 2024. [[PDF](https://arxiv.org/pdf/2311.17528.pdf)] [[Project]](https://hidiffusion.github.io/) [[Code]](https://github.com/megvii-research/HiDiffusion)<br>

**Training-Free Consistent Text-to-Image Generation.**<br>
*Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2402.03286.pdf)] [[Project]](https://consistory-paper.github.io/)<br>

**FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis.**<br>
*Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, Hongsheng Li.*<br>
ECCV 2024. [[PDF](https://arxiv.org/pdf/2403.12963.pdf)] [[Code]](https://github.com/LeonHLJ/FouriScale)<br>

## Image Manipulation

**360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation.**<br>
*Hai Wang, Jing-Hao Xue.*<br>
WACV 2025. [[PDF](https://arxiv.org/abs/2409.08397)] [[Project](https://littlewhitesea.github.io/360PanT.github.io/)] [[Code]](https://github.com/littlewhitesea/360PanT)<br>

**OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable Diffusion Model.**<br>
*Runyi Li, Xuhan Sheng, Weiqi Li, Jian Zhang.*<br>
ECCV 2024. [[PDF](https://arxiv.org/abs/2404.10312)] [[Project](https://lirunyi2001.github.io/projects/omnissr/)] [[Code]](https://github.com/LiRunyi2001/OmniSSR)<br>

**Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing.**<br>
*Vadim Titov, Madina Khalmatova, Alexandra Ivanova, Dmitry Vetrov, Aibek Alanov.*<br>
arxiv 2024. [[PDF](https://arxiv.org/abs/2409.01322)] [[Code]](https://github.com/FusionBrainLab/Guide-and-Rescale)<br>

**TALE: Training-free Cross-domain Image Composition via Adaptive Latent Manipulation and Energy-guided Optimization.**<br>
*Kien T. Pham, Jingye Chen, Qifeng Chen.*<br>
ACM MM 2024. [[PDF](https://arxiv.org/abs/2408.03637)] [[Project](https://tkpham3105.github.io/tale/)] [[Code]](https://github.com/tkpham3105/TALE)<br>

**Faster Diffusion via Temporal Attention Decomposition.**<br>
*Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, JÃ¼rgen Schmidhuber.*<br>
arXiv 2024. [[PDF](https://arxiv.org/abs/2404.02747v2)] [[Code](https://github.com/HaozheLiu-ST/T-GATE)]<br>

**DiffUHaul: A Training-Free Method for Object Dragging in Images.**<br>
*Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, Chunhua Shen.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.01594)] [[Project](https://omriavrahami.com/diffuhaul/)]<br>

**Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model.**<br>
*Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao.*<br>
Siggraph 2024. [[PDF](https://arxiv.org/pdf/2405.10316)] [[Project](https://analogist2d.github.io/)] [[Code]](https://github.com/edward3862/Analogist)<br>

**ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion.**<br>
*Ziyue Zhang, Mingbao Lin, Rongrong Ji.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.17230)]<br>

**CutDiffusion: A Simple, Fast, Cheap, and Strong Diffusion Extrapolation Method.**<br>
*Mingbao Lin, Zhihang Lin, Wengyi Zhan, Liujuan Cao, Rongrong Ji.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.15141.pdf)] [[Project]](https://github.com/lmbxmu/CutDiffusion)<br>

**FreeDiff: Progressive Frequency Truncation for Image Editing with Diffusion Models.**<br>
*Wei Wu, Qingnan Fan, Shuai Qin, Hong Gu, Ruoyu Zhao, Antoni B. Chan.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.11895.pdf)] <br>

**FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition.**<br>
*Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, Bolei Zhou.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2312.07536.pdf)] [[Project](https://genforce.github.io/freecontrol/)] [[Code]](https://github.com/genforce/freecontrol)<br>

**Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation.**<br>
*Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel.*<br>
CVPR 2023. [[PDF](https://arxiv.org/pdf/2211.12572.pdf)] [[Project](https://pnp-diffusion.github.io/)] [[Code]](https://github.com/MichalGeyer/plug-and-play)<br>

## Video Generation

**MotionMaster: Training-free Camera Motion Transfer For Video Generation.**<br>
*Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, Lizhuang Ma.*<br>
ACM MM 2024. [[PDF](https://arxiv.org/abs/2404.15789)] [[Code]](https://github.com/sjtuplayer/MotionMaster)<br>

## Video Editing

**Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices.**<br>
*Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli.*<br>
ICML 2024. [[PDF](https://arxiv.org/pdf/2405.12211)] [[Project](https://matankleiner.github.io/slicedit/)] [[Code]](https://github.com/fallenshock/Slicedit)<br>

**FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing.**<br>
*Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, Sen He.*<br>
ICLR 2024. [[PDF](https://arxiv.org/pdf/2310.05922.pdf)] [[Project](https://flatten-video-editing.github.io/)] [[Code]](https://github.com/yrcong/flatten)<br>

**TokenFlow: Consistent Diffusion Features for Consistent Video Editing.**<br>
*Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel.*<br>
ICLR 2024. [[PDF](https://arxiv.org/pdf/2307.10373.pdf)] [[Project](https://diffusion-tokenflow.github.io/)] [[Code]](https://github.com/omerbt/TokenFlow)<br>




